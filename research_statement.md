---
title: Research Statement
layout: page
---

My research interest primarily lies in building a syntactically- and semantically-aware compositional model of language. Let's break this down.

Let’s start with compositionality. Traditional language models have a very simplistic formulation — they treat language as finite state automaton with a Markovian assumption. This is a considerably strong assumption while modeling something as complex as language. Language is compositional in nature, and we must use this compositionality while building probability distribution over words and phrases. The computational linguistics community has been working for years on formalizing the structure of language in the form of grammars. A step in the right direction would be to look beyond the traditional model’s underlying sequential tree assumption and to use these compositional frameworks for modeling language.

Now, let's come to the semantic and syntactic awareness of the model, rather that of the representation. Let's start with two sentences --- "*A horse runs a steeplechase*" and "*A box runs a steeplechase*". An ideal language model, given these two phrases, should be able to identify that the former is way more semantically likely than the latter. We can formulate the task at hand as a representation learning problem. Starting with words, the resultant phrases should be recursively embedded back in latent space such that the first sentence falls in a region of high probability, whereas the second sentence falls in the lower probability space. Interesting this to note here is that the suffix "*runs a steeplechase*" is same for both the sentences. Ideally, this model would have a semblance of semantic awareness to understand that the prefix "*A horse*" has a higher affinity towards prepending to the phrase "*runs a steeplechase*" than the phrase "*A box*". We can make a similar argument about syntactic awareness by considering a pair of sentences, one of which is grammatically accurate and other one isn't. The embedding of the grammatically correct one should fall in a region of higher probability compared to the inaccurate one. 


Traditional n-gram based approaches particularly fall short on this semantics awareness objective. The reasons are twofold: 1.) lack of any implicit measure of relatedness or context based clustering in the discrete probability space, and 2.) their inability to capture longer context dependencies due to Markovian boundary restrictions. Distributional models like feedforward neural network based language models (NNLMs) and recurrent network based language models (RNNLMs) have tried to solve a part of this problem. They have done this by building a smooth probability distribution in continuous embedding space. The embeddings learned by these models have shown implicit clustering of the similar words and affine relations among the related words. The pertinent question to ask here is: Is this enough to differentiate between two syntactically similar but semantically different sentences? The answer is probably **No**.  For one, the embedding of the word in these models are causal i.e. they only depend on the past sequence of words, whereas the context with biggest influences on the current word's meaning may lie in words that are to follow. Secondly, the composition structure for the traditional models is a fixed sequential tree. This means that the impact of a preceding word on the current word embedding is a monotonically decreasing function of the distance between them. This, to me, is not a valid assumption. Recently, tree-based RNN (Recursive neural networks) based approaches have used the compositional structure for language tasks like sentiment analysis, sentence completion, and parsing. This is a significant step in right direction, but the underlying structural assumption remains, just that the composition structure is now a parse tree instead of a sequential one.

In my master's thesis, I attempted to address these lacunae in the traditional approaches by proposing a distributed compositional language model.  I started this endeavor by showing how n-gram models have an implicit sequential tree assumption. I followed this up by building a model that overcame underlying structural assumption by marginalizing over all possible trees. This was done by modeling sentence as an expectation (on the composition tree) over a joint probability of words and the composition structure. The result of this work was an elegant and efficient compositional language modeling framework that recursively builds sentences from words and phrases, and is free of any underlying structural assumptions. I hypothesize that the compositional model I have proposed would do a better job at building ***robust*** embeddings. This is because the composition function and embeddings it learns considers the whole sentence instead of the last few words. Additionally, this compositional model is free from any structural assumptions as it does not rely on a single compositional structure instead it considers all possible trees for a sentence.

Now, why would we want to do this? I firmly believe a ***robust*** compositional model and the embeddings generated by it can be used to build a better classifier for discriminative tasks like sentiment analysis, paraphrase detection, sentence completion, and parsing. The assumption here is that most of the heavy lifting would already be taken care of by the representation and the composition function. Adding a shallow network with a classification layer on top should be able to match the performance of the current state of the art approaches.

This brings us to the question, what is the best way to study these probability distribution functions, compositional structures, and resultant phrasal embeddings? The best way to look at the probability distribution is as a (smooth) function of the embeddings and its compositional structure. For phrasal embeddings, the best perspective would be to see it as a manifold learning problem. The objective, in this case, would be to embed words and phrases on or ***near***  a manifold that follows a set of predetermined ***desirable*** intrinsic properties. This would need a study of both the design of a composition function as well as understanding the impact of compositional structure on these embeddings. Additionally, we would need to understand the additional constraints we need to add to this generative compositional network to shape the manifold to have these ***desired*** properties.


Before embarking upon the journey to achieve any of the above-stated objectives, we need to ask the following questions: What do we mean by a ***robust*** representation? How do we define the ***desirable*** properties of the manifold? How do we define the ***neighborhood*** on this manifold? Having defined robustness of the embeddings and desirable properties of the space, what kind of evaluation frameworks we need to compare these compositional frameworks and embeddings to one another? How would we define the global and task-specific metrics to evaluate these manifolds and composition functions? 

I believe answers to these questions can considerably advance state of the art in the field of natural language understanding, speech recognition, machine translation, and information retrieval,  and can take us a step closer to the overarching goal of helping computers truly understand human language.