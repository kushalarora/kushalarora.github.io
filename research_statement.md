---
title: Research Statement
layout: page
---

My research interest primarily lies in building a syntactically and semantically aware compositional model of language. Let's break this down.

Let's start with language modeling. Traditional language models have a very simplistic formulation, they treat language as finite state automaton with a Markovian assumption. This is a very strong assumption for modeling something inherently as complex as language. Language is recursive in nature, and this recursivity should be used while building probability distribution over word and phrases. In my thesis I addressed this lacunae. I did this by showing how n-gram models have  an implicit sequential tree assumption. We then overcame this structural assumption by marginalizing over all possible underlying trees. The result of this work was an elegant and efficient compositional language modeling framework that recursively builds sentences from words and phrases, and is free of any underlying structural assumptions.

Now, let's come to the semantic and syntactic awareness of the model, rather that of the representation. An ideal language model, given two phrases "*A horse runs a steeplechase*" and "*A box runs a steeplechase*", should be able to identify that the former is way more likely to be a correct as compared to the latter. An ideal solution to this, according to me, will be a compositional one, the one where words and phrases would come together in such a way that the probability distribution of the resultant phrase is a function of both its syntactic and semantic cues. What this means is that the compositional solution will build phrases and sentences in such a way that the former sentence will fall in a higher probability space whereas the latter will fall in a space with lower probability values. Counting based n-gram models will fail miserably at this job, reason being the lack of any kind of implicit measure of relatedness or context based clustering in the discrete probability space and their inability to capture longer context dependencies due to Markovian boundary restrictions. Distributional models like feedforward neural network based language models (NNLMs) and recurrent network based language models (RNNLMs) solve a part of this problem. They do this by building a smooth probability distribution over a continuous space. The embeddings learnt by these models have shown implicit clustering of the similar words and affine relations among the related words, but is this enough to differentiate the two syntactically similar but semantically different sentences? The answer is probably no.  For one, the embedding of the word in these model are causal i.e. they only depend upon the past sequence of words, whereas the context that influences the word's meaning the most may lie in words that are to follow. Secondly, for RNNs, assuming the history to be a phrasal embedding of last *n-1* words, the composition structure is a fixed sequential tree. This means that the impact of a preceding word on the current word embedding is monotonically decreasing function of the distance between them which, to me, is not a valid assumption.   I hypothesize that the compositional model I have proposed would do better job at building more robust embeddings. This is because the composition function it learns considers the whole sentence instead of a last few words and also, due to the fact that it considers all possible parse tree of the sentence and doesn't have a strong underlying structural assumption.

Now, why would we want to do this? I strongly believe a robust compositional model and the embeddings generated by it can be used to build better classifiers for many discriminative NLU tasks like sentiment analysis, paraphrase detection, sentence completion and parsing to name a few. The assumption here is that most of the heavy lifting would already be taken care of by the representation and the composition function, and a simple classification layer on top of this should be able to do better than the current state of the art approaches.

This brings us to the question, what is the best way to study these compositional structures and resultant phrasal embeddings? I firmly believe the best way to look at this is to see this as a manifold and embedding function learning problem. For example, starting with a uniformly distributed vocabulary, we can study the impact of embedding functions on the underlying manifold, its ability to embed semantically and syntactically probable phrases in higher probability space, and desirable latent space properties like topical clustering and geodesic distances between synonyms and antonyms on this manifold. An equally important and interesting area is the evaluation of embeddings themselves. What do we mean by robust representation? Questions like, what are the desirable properties for these embeddings? What are the metrics to compare such compositional frameworks and embeddings? What are the additional constraints that can be added to improve these metrics?

I believe answers to these questions can considerably advance the state of art in the field of natural language understanding and related fields like speech recognition, machine translation and  information retrieval, and take us a step closer to goal of helping computers truly understand human languages.