---
title: Research Statement
layout: page
---

My research interest primarily lies in building a syntactically and semantically aware compositional model of language. Let's break this down.

Let's start with language modeling. Traditional language model have a very simplistic formulation, they treat language as finite state automaton with Markovian assumption. This is a very strong assumption for modeling something inherently as complex as language. Language is recursive in nature, and this recursivity should be used while building probability distribution over word and phrases. In my thesis I addressed this lacunae. I did this by showing how n-gram models have  an implicit sequential tree assumption. We then overcame this structural assumption by marginalizing over all possible underlying trees. The result of this work was an elegant and efficient compositional language modeling framework that uses distributional representation of words to compose phrases and sentences, and to build a probability distribution on them.

Now, let's come to the semantic and syntactic awareness of the model, rather that of the representation. An ideal language model, given two phrases, "*A Horse runs a steeplechase*" and "*An Alligator runs a steeplechase*", should be able to identify that the former is way more likely to be a correct as compared to the latter. Counting based n-gram models fail miserably at this job, reason being the  lack of any kind of implicit measure of relatedness or context based clustering in the discrete probability space. Distributional models like feedforward neural network based language models (NNLMs) and recurrent network based language models (RNNLMs) solve a part of this problem by building a smooth probability distribution over a continuous space. The embeddings learnt by these models have shown implicit clustering of similar words and affine relations among related words, but is this enough to differentiate the two syntactically similar but semantically different sentences? The answer is no.  The problem above needs a compositional solution where we build and embed phrases and sentences such that the former sentence falls in a high probability space and the latter falls in a low one. I hypothesize that the compositional model I proposed would do better at this job as it learns a composition function for the phrasal embeddings that consider the whole sentence structure instead of a last few words.

Now, why would we want to do this? I strongly believe a robust compositional model and the embeddings generated by it can be used to build better classifiers for many discriminative NLU tasks like sentiment analysis, paraphrase detection, sentence completion and parsing to name a few. The assumption here is that most of the heavy lifting would already be taken care of by the representation and composition function, and a simple classification layer on top of this should be able to do better than the current state of the art approaches.

 I firmly believe the best way to look at this is to see this as a manifold and embedding function learning problem. I am also interested in the properties of these embedding functions and their impact on the latent space manifold. As these composition functions are nothing but a deep neural network, my another research interest is better pre-training, optimization and regularization techniques for deep networks. 
 
 In addition to this, we need a  define what a robust representation means and what are the metrics to evaluate these compositional frameworks and embeddings. This is another area that interests me and I would like to work on.