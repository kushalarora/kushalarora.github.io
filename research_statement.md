---
title: Research Statement
layout: page
---

My research interest primarily lies in building a syntactically and semantically aware compositional model of language. Let's break this down.

Let's start with language modeling. Traditional language models have a very simplistic formulation, they treat language as finite state automaton with a Markovian assumption. This is a very strong assumption for modeling something inherently as complex as language. Language is recursive in nature, and this recursivity should be used while building probability distribution over word and phrases. The computational linguistics community has been working for years on formalizing the underlying structure of language in the form of grammars. A step in the right direction would be to look beyond simple frequency estimation-based methods and to use these compositional frameworks to assign the probability to words and sentences.

Now, let's come to the semantic and syntactic awareness of the model, rather that of the representation. An ideal language model, given two phrases "*A horse runs a steeplechase*" and "*A box runs a steeplechase*", should be able to identify that the former is way more likely to be a correct as compared to the latter. According to me, an ideal solution to this should be a compositional in nature, one where words and phrases would come together in such a way that the probability distribution of the former resultant phrase mapped to a higher probability space whereas the latter should fall in a space with lower probability values. Counting based n-gram models fail miserably at this job, the reason being the lack of any implicit measure of relatedness or context based clustering in the discrete probability space and their inability to capture longer context dependencies due to Markovian boundary restrictions. Distributional models like feedforward neural network based language models (NNLMs) and recurrent network based language models (RNNLMs) solve a part of this problem by building a smooth probability distribution in continuous embedding space. The embeddings learned by these models have shown implicit clustering of the similar words and affine relations among the related words, but is this enough to differentiate the two syntactically similar but semantically different sentences? The answer is probably no.  For one, the embedding of the word in these model are causal i.e. they only depend on the past sequence of words, whereas the context with biggest influences on the word's meaning may lie in words that are to follow. Secondly, for RNNs the composition structure is a fixed sequential tree. This means that the impact of a preceding word on the current word embedding is a monotonically decreasing function of the distance between them which, to me, is not a valid assumption. 

In my thesis, I attempted to address these lacunae in the traditional approach to language modeling by proposing a distributed compositional language model. I did this by modeling sentence expected probability as a joint probability of words and the composition structure. I  showed how an n-gram models have an implicit sequential tree assumption and then overcame this structural assumption by marginalizing over all possible underlying trees. The result of this work is an elegant and efficient compositional language modeling framework that recursively builds sentences from words and phrases, and is free of any underlying structural assumptions. I hypothesize that the compositional model I have proposed would do better job at building *"robust"* embeddings. This is because the composition function and embeddings it learns considers the whole sentence instead of the last few words and also, it does not have a strong structural assumption as it considers all possible parse trees for the sentence.

Now, why would we want to do this? I firmly believe a *"robust"* compositional model and the embeddings generated by it can be used to build better classifiers for many discriminative NLU tasks like sentiment analysis, paraphrase detection, sentence completion and parsing to name a few. The assumption here is that most of the heavy lifting would already be taken care of by the representation and the composition function, and a simple classification layer on top of this should be able to do better than the current state of the art approaches.

This brings us to the question, what is the best way to study these compositional structures and resultant phrasal embeddings? I believe the best way to look at this is to see it as a manifold and embedding function learning problem where the word and phrases lie on a manifold which follows *"desirable"* intrinsic properties. The composition function we learn should be able to embed words either on or *"near"* this manifold. This can be achieved by first studying the compositional structure and function we have and looking at the how are they embedding words and phrases in this space. Secondly, tweaking the composition function, structure, and the loss function to shape the manifold to have the desired properties. 

As the embedding are also influenced by the composition structure of the sentence, it would be useful to study the effect of different grammars on the embeddings and how they shape the underlying manifold. Also, an important line of inquiry would be to learn grammars which will lead to a better embedding and why is it? Additionally, do these grammars result in better performance on the real world applied tasks?

An equally important and exciting area is the evaluation of embeddings themselves. What do we mean by a *"robust"* representation? Also, what are these *"desirable"* properties for these embeddings? Having defined *"robustness"*  of the embeddings and *"desirable"* properties of the space, what are the metrics to compare such compositional frameworks and embeddings? Moreover, What are the additional constraints that can be added to improve on these metrics? 

I believe answers to these questions can considerably advance state of the art in the field of natural language understanding and related fields like speech recognition, machine translation, and information retrieval, and take us a step closer to the goal of helping computers truly understand human language.