---
layout: page
title: Projects
---

* #### *Compositional Language Modeling* ####
 Traditional language models treat language as a linear chain on words. In my master’s thesis I challenged this assumption and proposed a model that uses underlying compositional structure for modeling language. This is done by marginalizing the joint probability of sentence and composition tree. Conditional probability given the structure was modeled using the distributional representation similar to neural network based models. We report more than 100% improvement in on a new evaluation metric, Contrastive Entropy, over the state of the art RNNLM.

    [[pdf](https://arxiv.org/submit/1522648/view), [code](http://github.com/kushalarora/CompositionalLM.git)]

* #### *Contrastive Entropy: A new metric for evaluating unnormalized level language models* ####
    Perplexity is an unsuitable metric for unnormalized language models due to its reliance on exact probabilities. Other extrinsic metrics like WER are costly to calculate and need extensive setup. As part of my thesis, I also proposed a new discriminative metric to evaluate unnormalized sentence level language models. The intuition here is to capture the model’s ability to differentiate between a test sentence and its distorted version, the one that is less likely to be generated by the language source.

    [[pdf](http://arxiv.org/pdf/1601.00248v2.pdf)]

* #### *Sentence Level Recurrent Neural Network* ####
    To prove the efficacy of our new metric, I reformulated RNN as a sentence level compositional model trained discriminatively to differentiate a sentence from the training set and a slightly distorted version of it. On our new metric we reported considerable gains over RNN when this model is trained at lower level of distortion(10%).


    [[pdf](http://arxiv.org/pdf/1601.00248v2.pdf), [code](http://github.com/kushalarora/sentenceRNN.git)]

* #### *Comparative Evaluation of Manifold Learning Algorithms* ####
    Implemented the state of the art dimensionality reduction algorithms in python using Scipy and compared them on
four data sets, namely RaceSpace, Digits, Faces and Swiss Roll. The project was an individual effort and done as a course project for Advanced Machine Learning class.

    <em><strong>Algorithms implemented</strong>: Local Linear Embedding, ISOMap, Laplacian Eigenmaps, Hessian LLE, Local Tanget Space Analysis, Stochastic Neighborhood Embedding</em>


    [[pdf](/assets/AMLProjectReport.pdf), [code](https://github.com/kushalarora/ManifoldAlgorithms.git)]

* #### Comparative Evaluation of Supervised Learning Algorithms ####
Built a generic framework to run a list of Supervised Learning Algorithms in Python using scikit­-learn and Theheano. The framework was used to do a comparative study on following data sets: Wisconsin Breast Cancer, Iris, Higgs, OCR and Hand Writing Recognition across a range of supervised learning algorithms. This project was done in a team of three for Machine Learning class.

    <em><strong>Algorithms evaluated</strong>: Multi Layer Perceptron, Stacked Auto Encoders, Deep Belief Network, Support Vector Machine, Random Forest, Decision Tree, AdaBoost Decision Tree. </em>

    [[code](https://github.com/kushalarora/SupervisedMLAlgorithms.git)]

* #### *Ontology Alignment for Knowledge Bases* ####
    Implemented and evaluated PARIS, an ontology alignment technique that uses web text based interlingua for aligning relations and entities. Ontologies for Freebase, NELL and Yago were mapped to each other using label propagation algorithm. This project was done as a independent study in Data Science Lab under Dr. Daisy Wang and was a part of larger objective to build a master KB for the lab.


    [[pdf](/assets/pidgin.pdf), [code](https://github.com/kushalarora/pidgin.git)]
