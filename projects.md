---
layout: page
title: Projects
---

### Compositional Language Modeling ###
Traditional language models treat language as a linear chain on words. In my Master’s thesis I challenged this assumption and proposed a model that uses underlying compositional structure for modeling language. This is done by marginalizing the joint probability of sentence and composition tree. Composition trees were generated using PCFGs and marginalization was carried out using the Inside algorithm. Conditional probability given the structure was modeled using the distributional representation similar to neural network based models. We report more than 100% improvement in Constrastive Entropy over RNNLM on a toy data set.


### Contrastive Entropy: A new metric for evaluating unnormalized level language models ###
Perplexity is an unsuitable metric for sentence level language model due to its reliance on exact probabilities. Other extrinsic metrics like WER are costly to calculate and need extensive setup. As part of my thesis, I also proposed a new discriminative metric to evaluate sentence level language models. The intuition here is to capture the model’s ability to differentiate between a good sentence and a bad sentence, one that cannot be generated by the language source.

### Sentence Level Recurrent Neural Network ###
To prove the efficacy of our new metric, I reformulated RNN as a sentence level compositional model trained discriminatively to differentiate a sentence from the training set and a slightly distorted version of it. On our new metric we reported considerable gains over RNN when this model is trained at lower level of distortion(10%).


### Comparative evaluation of Manifold Learning Algorithms ###
Implemented the state of the art dimensionality reduction algorithms in python using scipy and compared them on
four data sets, namely RaceSpace, Digits, Faces and Swiss Roll. The project was an individual effort and done as a course project for Advanced Machine Learning class.

<strong><em>Algorithms implemented</em></strong>: L ocal Linear Embedding, ISOMap, Laplacian Eigenmaps, Hessian LLE, Local Tanget Space Analysis, Stochastic Neighborhood Embedding

### Comparative evaluation of Supervised Learning Algorithms ###
Built a generic framework to run a list of Supervised Learning Algorithms in Python using Scikit­Learn and Theano. The framework was used to do a comparative study on following data sets: Wisconsin Breast Cancer, Iris, Higgs, OCR and Hand Writing Recognition across a range of supervised learning algorithms. This project was done in a team of three for Machine Learning class.

<strong><em>Algorithms evaluated</em></strong>: Multi Layer Preceptron, Stacked Auto Encoders, Deep Belief Network, Support Vector Machine, Random Forest, Decision Tree, AdaBoost Decision Tree.

### Ontology Alignment for Knowledge Bases ###
Implemented and evaluated PARIS, an ontology alignment technique that uses web text based interlingua for aligning relations and entities. Ontologies for Freebase, NELL and Yago were mapped to each other using label propagation algorithm.ThisprojectwasdoneasaindependentstudyinDataScienceLabunderDrDaisyWangand wasapartof larger objective to build a master KB for the lab.
