---
layout: page
title: (An Incomplete) Reading List
---

#### Rants: ####
* [On Chomsky and the Two Cultures of Statistical Learning.](http://norvig.com/chomsky.html)
* [*On our best behvior* by Hector J. Levesque.](http://www.cs.toronto.edu/~hector/Papers/ijcai-13-paper.pdf)

#### Language Modeling: ####
* [Recurrent neural network based language model.](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS*pdf)
* [Extensions of Recurrent Neural Network Language Model.](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_*pdf)
* [A Neural Probabilistic Language Model.](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
* [An empirical study of smoothing techniques for language modeling](http://www.speech.sri.com/projects/srilm/manpages/pdfs/chen-goodman-tr-10-*pdf)
* [Deep Learning, NLP, and Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
* [Top-down Tree Long Short-Term Memory Networks](https://arxiv.org/pdf/1511.00060.pdf)
* [Immediate-Head Parsing for Language Models](http://www.aclweb.org/anthology/P01-1017)
* [A Bit of Progress in Language Modeling](http://research.microsoft.com/en-us/um/redmond/groups/srg/papers/2001-joshuago-tr72.pdf)
* [A Scalable Hierarchical Distributed Language Model](http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf)
* [Hierarchical Probabilistic Neural Network Language Model](http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf)
* [A Convolutional Neural Network for Modelling Sentences](https://arxiv.org/pdf/1404.2188v1)
* [A Structured Language Model](https://www.aclweb.org/anthology/P/P97/P97-*pdf)

#### Parsing: ####

#### Compositional Models: ####
* [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)
* [Composition in Distributional Models of Semantics](https://pdfs.semanticscholar.org/745d/86adca56ec50761591733e157f84cfb19671.pdf)
* [Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space](http://clic.cimec.unitn.it/marco/publications/bz-adj-com-emnlp10.pdf)
* [Vector-based Models of Semantic Composition](http://anthology.aclweb.org/P/P08/P08-1.pdf#page=280)
* [Mathematical foundations for a compositional distributional model of meaning](http://arxiv.org/pdf/1003.4394)
* [Semantic compositionality through recursive matrix-vector spaces](http://ttic.uchicago.edu/~haotang/speech/SocherHuvalManningNg_EMNLP2012.pdf)
* [A Comparison of Vector-based Representations for Semantic Composition](http://www.aclweb.org/anthology/D12-1050)
* [Representing Meaning with a Combination of Logical and Distributional Models](https://arxiv.org/pdf/1505.06816v5)


#### Neural Networks: ####
* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [Gated Feedback Recurrent Neural Networks](http://jmlr.csail.mit.edu/proceedings/papers/v37/chung15.pdf)

#### Representation Learning: ####

#### Language Embeddings: ####
* [Efficient Estimation of Word Representations in Vector Space](http://arxiv.org/pdf/*3781v*pdf)
* [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
* [Linguistic Regularities in Continuous Space Word Representations.](http://www.aclweb.org/anthology/N13-1#page=784)
* [Natural Language Processingfrom Scratch](https://arxiv.org/pdf/1103.0398v1.pdf)
* [GloVe: Global Vectors for Word Representation Jeffrey](http://nlp.stanford.edu/pubs/glove.pdf)
* [Learning to Understand Phrases by Embedding the Dictionary](https://arxiv.org/pdf/1504.00548v4.pdf)
* [Distributed Representations of Sentences and Documents.](http://www.jmlr.org/proceedings/papers/v32/le*pdf)
* [Zero-shot learning by convex combination of semantic embeddings](https://arxiv.org/pdf/**pdf)
* [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/**pdf)

#### Manifold Learning: ####
* [An Introduction to Locally Linear Embedding](https://www.cs.nyu.edu/~roweis/lle/papers/lleintro.pdf)
* [IsoMap](http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf)
* [Laplacian eigenmaps](https://pdfs.semanticscholar.org/989a/f45f8242b96cecb91d48b85620e7322e4aa7.pdf)
* [Diffusion maps](http://inside.mines.edu/~whereman/talks/delaPorte-Herbst-Hereman-vanderWalt-DiffusionMaps-PRASA2008.pdf)
* [Neighborhood Preserving Embedding](http://people.cs.uchicago.edu/~xiaofei/conference-15.pdf)
